{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = Augmentor.Pipeline(\"train/not_empty\")\n",
    "#p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
    "#p.sample(52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the pretrained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg16_conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Freeze the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_vgg16_conv.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150, 150\n",
    "train_data_dir = 'train2' #40 images from each class for training\n",
    "model_weights_file = 'vgg16-xfer-weights.h5'\n",
    "nb_train_samples = 80\n",
    "nb_val_samples = 4\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a classification model on top of Base Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(img_width, img_height, 3))\n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "x = Flatten()(output_vgg16_conv)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "model = Model(input=input, output=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                multiple                  14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                524352    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 15,243,330\n",
      "Trainable params: 528,642\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_width, img_height), \n",
    "                                                    batch_size=2, class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 34s 851ms/step - loss: 0.8841 - acc: 0.5250\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 35s 882ms/step - loss: 0.6331 - acc: 0.7000\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 36s 912ms/step - loss: 0.5403 - acc: 0.7875\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 34s 850ms/step - loss: 0.4110 - acc: 0.8250\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 34s 850ms/step - loss: 0.4155 - acc: 0.8500\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 36s 893ms/step - loss: 0.3156 - acc: 0.8750\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 41s 1s/step - loss: 0.2520 - acc: 0.9125\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 37s 937ms/step - loss: 0.2849 - acc: 0.8750\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 38s 953ms/step - loss: 0.3387 - acc: 0.9250\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 36s 910ms/step - loss: 0.2223 - acc: 0.9250\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 36s 894ms/step - loss: 0.1785 - acc: 0.9250\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 37s 920ms/step - loss: 0.1035 - acc: 0.9625\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 39s 972ms/step - loss: 0.1767 - acc: 0.9375\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 36s 907ms/step - loss: 0.1521 - acc: 0.9500\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 36s 892ms/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 37s 920ms/step - loss: 0.1212 - acc: 0.9750\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 37s 931ms/step - loss: 0.1082 - acc: 0.9875\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 38s 943ms/step - loss: 0.1421 - acc: 0.9500\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 43s 1s/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 37s 932ms/step - loss: 0.0958 - acc: 0.9625\n",
      "Training Completed!\n"
     ]
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint(model_weights_file, monitor='acc', mode = 'max', save_best_only=True)]\n",
    "\n",
    "history = model.fit_generator( train_generator, callbacks = callbacks, samples_per_epoch=nb_train_samples, \n",
    "                              nb_epoch=nb_epochs, nb_val_samples=nb_val_samples)\n",
    "\n",
    "print('Training Completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8076923076923077\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "#test folder contains 13 images from each class \n",
    "\n",
    "label = ['empty', 'not_empty']\n",
    "\n",
    "for filename in glob.glob('/home/divyaansh/Downloads/OneDrive-2019-10-10/test/empty/*.jpg'):\n",
    "    img = image.load_img(filename, target_size=(150, 150))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    features = model.predict(x)\n",
    "    ind = np.where(features >= 0.5)[1][0]\n",
    "    if(label[ind] == 'empty'):\n",
    "        correct+=1\n",
    "    total+=1\n",
    "\n",
    "for filename in glob.glob('/home/divyaansh/Downloads/OneDrive-2019-10-10/test/not_empty/*.jpg'):\n",
    "    img = image.load_img(filename, target_size=(150, 150))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    features = model.predict(x)\n",
    "    ind = np.where(features >= 0.5)[1][0]\n",
    "    if(label[ind] == 'not_empty'):\n",
    "        correct+=1\n",
    "    total+=1    \n",
    "print(\"Accuracy:\",correct/total);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vgg16_model.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(model,'vgg16_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from PIL import Image\n",
    "#import PIL.Image\n",
    "\n",
    "#import pytesseract\n",
    "#from pytesseract import image_to_string\n",
    "##pytesseract.pytesseract.tesseract_cmd = 'sudo /usr/local/lib/python3.6/dist-packages'\n",
    "##print image_to_string(Image.open('empty.jpg'))\n",
    "#image_to_string(Image.open('img.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
